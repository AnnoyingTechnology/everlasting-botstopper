##################### Spam Bot Blocking ##########################

# SPAM blocker v1.6 by Charlton Trezevant

# ---ABOUT---
# These are some configuration rules for Lighttpd that block common spam crawlers and search engines that don't 
# honor robots.txt. I use this over at http://www.ctis.me to block annoying bots and such that I constantly see in my webserver access logs, and it functions extremely well.

# Blocking spam crawlers is important because it will help make sure that personal information that may be on your site, such as phone
# numbers or emails, don't get picked up by spammers. It also keeps your webserver from serving web pages to robots who often make many
# requests at a time, thus freeing bandwidth and system resources for legitimate visitors.

# ---INSTALLATION---
# Installation is simple, simply include this file (recommended) or copy and paste this into your /etc/lighttpd/lighttpd.conf, 
# and restart/reload.
# For other webservers, these regular expressions should carry over, but I can't guarantee it. If in doubt, consult your documentation.

# ---SUGGESTIONS AND REPORTING---
# If you have any suggestions for new rules that should be added, or perhaps have questions or concerns, please leave a comment below
# or send an email to ct[ at] charltontrezevant.com

#Block spam referrers!
$HTTP["referer"] =~ "semalt.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "darodar.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "makemoneyonline.com|7makemoneyonline.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "buttons-for-website.com|buttons-for-your-website.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "social-buttons.com|free-share-buttons.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "best-seo-offer.com|best-seo-solution.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "buy-cheap-online.info" { url.access-deny =  ( "" ) }


#Block spam crawlers!

#Empty UA, or a -.
#This blocks a lot of spam crawlers and bad robots. 

$HTTP["useragent"] == "" {
    url.access-deny = ( "" )
}

$HTTP["useragent"] == "-" {
    url.access-deny = ( "" )
}

#Blocks the mass internet scanning tool https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "massscan" {
    url.access-deny = ( "" )
}

#Serve 403 to UAs w/ attempted perl injection
#Coming soon if I can figure out how to make lighttpd refuse connections, at the very least we can deny all URLs. 
$HTTP["useragent"] =~ "perl" {
    url.access-deny = ( "" )
}

#MJ12, used by majecticseo.com https://www.majestic12.co.uk/projects/dsearch/mj12bot.php
$HTTP["useragent"] =~ "MJ12bot" {
    url.access-deny = ( "" )
}

#IPS-agent by VeriSign http://www.spambotsecurity.com/forum/viewtopic.php?f=7&t=1453
$HTTP["useragent"] =~ "ips-agent" {
    url.access-deny = ( "" )
}

#Indy Library, a scanner commonly used to collect emails and so forth. 
$HTTP["useragent"] =~ "Indy Library" {
    url.access-deny = ( "" )
}

#China.Z, another nasty shellshock exploiter.
$HTTP["useragent"] =~ "China.Z" {
    url.access-deny = ( "" )
}

#5118, yet another spammy SEO company.
$HTTP["useragent"] =~ "5118.com" {
    url.access-deny = ( "" )
}

#NetCraft, another spammy SEO company http://www.netcraft.com/
$HTTP["useragent"] =~ "Netcraft" {
    url.access-deny = ( "" )
}

#Thunderstone, yet _another_ spammy SEO company 
$HTTP["useragent"] =~ "thunderstone" {
    url.access-deny = ( "" )
}

#Baidu
$HTTP["useragent"] =~ "Baidu|Baiduspider" {
    url.access-deny = ( "" )
}

#Those sneaky jerks! These IPs are also Baidu, because they appear not to respect robots.txt and 
#simply change the user-agents of their spiders so as to circumvent blocking. 
$HTTP["remoteip"] =~ "123\.125\.71\.*|220\.181\.108\.*" {
    url.access-deny = ( "" )
}

#Sogou
$HTTP["useragent"] =~ "Sogou" {
    url.access-deny = ( "" )
}

#YoDao
$HTTP["useragent"] =~ "YoudaoBot|YodaoBot" {
    url.access-deny = ( "" )
}

#Haosou
$HTTP["useragent"] =~ "HaoSouSpider|360Spider" {
    url.access-deny = ( "" )
}

#Lightspeed Systems
$HTTP["useragent"] =~ "LSSRocketCrawler|LightspeedSystems" {
    url.access-deny = ( "" )
}

#Always allow access to robots.txt, to everyone.
$HTTP["url"] =~ "^/robots.txt"{
    url.access-deny = ("disable")
}
##################### END Spam Bot Blocking ##########################