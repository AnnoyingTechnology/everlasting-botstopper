##################### Spam Bot Blocking ##########################

# Spam blocker v2.0 by Charlton Trezevant

# ---ABOUT---
# These are some configuration rules for Lighttpd that block common spam crawlers and search engines that don't 
# honor robots.txt. I use this over at http://www.ctis.me to block annoying bots and such that I constantly see in my webserver access
# logs, and it works extremely well.

# Blocking spam crawlers is important because it will help make sure that personal information that may be on your site, such as phone
# numbers or emails, don't get picked up by spammers. It also keeps your webserver from serving web pages to robots who often make many
# requests at a time, thus freeing bandwidth and system resources for legitimate visitors.
# Check out http://blog.ctis.me/2015/05/blocking-bad-bots-in-3-easy-steps-in.html for some more information.

# ---INSTALLATION---
# Installation is simple, simply paste this into a file named spam.conf in /etc/lighttpd/, then, 
# in your lighttpd configuration, add the line: include /etc/lighttpd/spam.conf
# Once finished, run the command sudo /etc/init.d/lighttpd reload && sudo /etc/init.d/lighttpd restart
# Or, you could copy and paste this into your /etc/lighttpd/lighttpd.conf, and run the above command.
# For other webservers, these regular expressions should carry over, but I can't guarantee it. If in doubt, consult your documentation.

# ---SUGGESTIONS AND REPORTING---
# If you have any suggestions for new rules that should be added, or perhaps have questions or concerns, please leave a comment below
# or send an email to ct[ at] charltontrezevant.com

# Disable Direct IP access to the server
# Enable this if you have a FQDN. This will stop most scanners 
# You may also want to consider redirecting to your FQDN rather than serving a 403.
$HTTP["host"] == "96.59.114.31" { url.access-deny = ( "" ) }

# Block spam referrers!
$HTTP["referer"] =~ "semalt.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "darodar.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "makemoneyonline.com|7makemoneyonline.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "buttons-for-website.com|buttons-for-your-website.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "social-buttons.com|free-share-buttons.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "best-seo-offer.com|best-seo-solution.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "buy-cheap-online.info" { url.access-deny =  ( "" ) }

### Block spam crawlers!
# Empty UA, -, or cURL (disabled by default).
#This blocks a lot of spam crawlers and bad robots. 
$HTTP["useragent"] == "" { url.access-deny = ( "" ) }
$HTTP["useragent"] == "-" { url.access-deny = ( "" ) }

# Serve 403 to UAs w/ attempted perl injection
# we can deny all URLs (minimizing access to potentially vulnerable apps)
$HTTP["useragent"] =~ "perl|China.Z" { url.access-deny = ( "" ) }

# Spammy SEO companies
# Some UAs sourced from http://www.iplists.com/non_engines.txt
# Thunderstone
# NetCraft http://www.netcraft.com/
# MJ12, used by majecticseo.com https://www.majestic12.co.uk/projects/dsearch/mj12bot.php
# 5118.com
# Lightspeed Systems
# IPS-agent by VeriSign http://www.spambotsecurity.com/forum/viewtopic.php?f=7&t=1453
# UA "TurnitinBot/1.4 http://www.turnitin.com/robot/crawlerinfo.html"
# UA "WebZIP/4.0 (http://www.spidersoft.com)"
# Indy Library, a scanner commonly used to collect emails and personal information.

$HTTP["useragent"] =~ "thunderstone|PageAnalyzer|Netcraft|5118.com|MJ12bot|dotbot|LSSRocketCrawler|LightspeedSystems|StatsInfo|ips-agent|TurnitinBot|SlySearch|ScoutAbout|RPT-HTTPClient|WebZIP|Webclipping.com|webrank|websquash.com|lwp-trivial|AESOP_com_SpiderMan|Zeus|Cyveillance|Lite\ Bot|flunky|Microsoft\ URL\ Control|NAMEPROTECT|NPBot|aipbot|WebFilter|InfoPath|Indy\ Library|SurveyBot" {
    url.access-deny = ( "" ) 
}

# Net Scanners, including https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "netscan.lekus.su|pdrlabs.net|massscan|ip138.com" { url.access-deny = ( "" ) }

# Some of these IPs are also Baidu, but none of them respect robots.txt and 
# simply change the user-agents of so as to circumvent blocking. Of course, we can fix that :)
# Blocked IPs:
# 123.125.71.x and 220.181.108.x - Baidu Spiders
# 202.46.*.* - Blocks bots @ ptr.cnsat.com.cn 

$HTTP["remoteip"] =~ "123\.125\.71\.*|220\.181\.108\.*|202\.46\.*.*" { url.access-deny = ( "" ) }


# Google Crawler Validation (disabled by default, uncomment to enable.)
# Unfortuntely, many spambots impersonate Google's web crawler (Googlebot).
#
# This config attempts to validate requests from Google user agents by comparing
# the ip address of the client to a list of known good IP blocks.
# 
# These IP blocks are retrieved through Google's publicly available lists 
# (https://support.google.com/a/answer/60764?hl=en, http://snurps.blogspot.com/2013/10/how-many-ip-addresses-does-google-have.html)
# with the command dig TXT +short _netblocks{,2,3}.google.com | tr ' ' '\n' | grep '^ip4:'
# Note that the whitelist is purposely as coarse as possible to prevent false negatives. 

$HTTP["useragent"] =~ "Google|Googlebot" {
  $HTTP["remoteip"] !~ "64\.*\.*\.*|66\.*\.*\.*|72\.*\.*\.*|74\.*\.*\.*|173\.*\.*\.*|207\.*\.*\.*|209\.*\.*\.*|216\.*\.*\.*" {
    url.access-deny = ( "" )
  }
}

# Baidu
$HTTP["useragent"] =~ "Baidu|Baiduspider" { url.access-deny = ( "" ) }

# Sogou
$HTTP["useragent"] =~ "Sogou" { url.access-deny = ( "" ) }

# YoDao
$HTTP["useragent"] =~ "YoudaoBot|YodaoBot" { url.access-deny = ( "" ) }

# Haosou
$HTTP["useragent"] =~ "HaoSouSpider|360Spider" { url.access-deny = ( "" ) }

# Always allow access to robots.txt, to everyone.
$HTTP["url"] =~ "^/robots.txt"{ url.access-deny = ("disable") }

##################### END Spam Bot Blocking ##########################