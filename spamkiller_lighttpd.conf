##################### Spam Bot + Referrer Blocking ##########################

# SPAM blocker v1.4 by Charlton Trezevant

#Block spam referrers.
$HTTP["referer"] =~ "^(http://(www\.)?semalt\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?semalt.semalt\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?darodar\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?forum.topic42900483.darodar\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?makemoneyonline\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?make-money-online.7makemoneyonline.com\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?buttons-for-website\.com)" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "^(http://(www\.)?buttons-for-your-website\.com)" { url.access-deny =  ( "" ) }

#Block spam crawlers!

#Empty UA, or a -.
#This blocks a lot of spam crawlers and bad robots. 

$HTTP["useragent"] == "" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

$HTTP["useragent"] == "-" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Blocks the mass internet scanning tool https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "massscan" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Kill connections w/ attempted perl injection
#Coming soon if I can figure out how to make lighttpd refuse connections, at the very least we can deny all URLs. 
$HTTP["useragent"] =~ "perl" {
    url.access-deny = ( "" )
}

#MJ12, used by majecticseo.com https://www.majestic12.co.uk/projects/dsearch/mj12bot.php
$HTTP["useragent"] =~ "MJ12bot" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#IPS-agent by VeriSign http://www.spambotsecurity.com/forum/viewtopic.php?f=7&t=1453
$HTTP["useragent"] =~ "ips-agent" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Indy Library, a scanner commonly used to collect emails and so forth. 
$HTTP["useragent"] =~ "Indy Library" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#China.Z, another nasty shellshock exploiter.
$HTTP["useragent"] =~ "China.Z" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}



#NetCraft, another spammy SEO company http://www.netcraft.com/
$HTTP["useragent"] =~ "Netcraft" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Thunderstone, yet _another_ spammy SEO company 
$HTTP["useragent"] =~ "thunderstone" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Baidu
$HTTP["useragent"] =~ "Baidu" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Those sneaky jerks! These next 3 are also Baidu, trying to sneakily crawl my site! 
$HTTP["useragent"] == "Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

$HTTP["useragent"] == "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:22.0) Gecko/20100101 Firefox/22.0" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

$HTTP["useragent"] == "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Sogou
$HTTP["useragent"] =~ "Sogou" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#YoDao
$HTTP["useragent"] =~ "YoudaoBot|YodaoBot" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Haosou
$HTTP["useragent"] =~ "HaoSouSpider|360Spider" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}
##################### END Spam Bot + Referrer Blocking ##########################
