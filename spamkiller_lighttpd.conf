##################### Spam Bot + Referrer Blocking ##########################

# SPAM blocker v1.5 by Charlton Trezevant

# ---ABOUT---
# These are some configuration rules for Lighttpd that block common spam crawlers and search engines that don't 
# honor robots.txt. I use this over at http://www.ctis.me to block annoying bots and such that I constantly see in my webserver access logs, and it functions extremely well.

# Blocking spam crawlers is important because it will help make sure that personal information that may be on your site, such as phone
# numbers or emails, don't get picked up by spammers. It also keeps your webserver from serving web pages to robots who often make many
# requests at a time, thus freeing bandwidth and system resources for legitimate visitors.

# ---INSTALLATION---
# Installation is simple, simply copy and paste this file into your /etc/lighttpd/lighttpd.conf, and restart/reload.
# For other webservers, these regular expressions should carry over, but I can't guarantee it. If in doubt, consult your documentation.

# ---SUGGESTIONS AND REPORTING---
# If you have any suggestions for new rules that should be added, or perhaps have questions or concerns, please leave a comment below
# or send an email to ct[ at] charltontrezevant.com

#Block spam referrers!
$HTTP["referer"] =~ "semalt.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "darodar.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "makemoneyonline.com|7makemoneyonline.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "buttons-for-website.com|buttons-for-your-website.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "social-buttons.com|free-share-buttons.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "best-seo-offer.com|best-seo-solution.com" { url.access-deny =  ( "" ) }
$HTTP["referer"] =~ "buy-cheap-online.info" { url.access-deny =  ( "" ) }


#Block spam crawlers!

#Empty UA, or a -.
#This blocks a lot of spam crawlers and bad robots. 

$HTTP["useragent"] == "" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

$HTTP["useragent"] == "-" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Blocks the mass internet scanning tool https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "massscan" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Kill connections w/ attempted perl injection
#Coming soon if I can figure out how to make lighttpd refuse connections, at the very least we can deny all URLs. 
$HTTP["useragent"] =~ "perl" {
    url.access-deny = ( "" )
}

#MJ12, used by majecticseo.com https://www.majestic12.co.uk/projects/dsearch/mj12bot.php
$HTTP["useragent"] =~ "MJ12bot" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#IPS-agent by VeriSign http://www.spambotsecurity.com/forum/viewtopic.php?f=7&t=1453
$HTTP["useragent"] =~ "ips-agent" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Indy Library, a scanner commonly used to collect emails and so forth. 
$HTTP["useragent"] =~ "Indy Library" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#China.Z, another nasty shellshock exploiter.
$HTTP["useragent"] =~ "China.Z" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#5118, yet another spammy SEO company.
$HTTP["useragent"] =~ "5118.com" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#NetCraft, another spammy SEO company http://www.netcraft.com/
$HTTP["useragent"] =~ "Netcraft" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Thunderstone, yet _another_ spammy SEO company 
$HTTP["useragent"] =~ "thunderstone" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Baidu
$HTTP["useragent"] =~ "Baidu" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Those sneaky jerks! These next 3 are also Baidu, trying to sneakily crawl my site! 
$HTTP["useragent"] == "Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

$HTTP["useragent"] == "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:22.0) Gecko/20100101 Firefox/22.0" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

$HTTP["useragent"] == "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Sogou
$HTTP["useragent"] =~ "Sogou" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#YoDao
$HTTP["useragent"] =~ "YoudaoBot|YodaoBot" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Haosou
$HTTP["useragent"] =~ "HaoSouSpider|360Spider" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}

#Lightspeed Systems
$HTTP["useragent"] =~ "LSSRocketCrawler|LightspeedSystems" {
    url.access-deny = ( "" )
    $HTTP["url"] =~ "^/robots.txt"{url.access-deny = ("disable")
    }
}
##################### END Spam Bot + Referrer Blocking ##########################
