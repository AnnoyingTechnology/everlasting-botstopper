##################### Spam Bot Blocking ##########################

# Spam blocker v2.3 by Charlton Trezevant

# Disable Direct IP access to the server
# Enable this if you have a FQDN. This will stop most scanners 
# You may also want to consider redirecting to your FQDN rather than serving a 403.
# This should be enabled but is set to ctis.me's IP by default so things aren't automagically broken if you forget ;)
$HTTP["host"] == "96.59.114.31" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Block spam referrers!
$HTTP["referer"] =~ "semalt.com|darodar.com|makemoneyonline.com|7makemoneyonline.com|buttons-for-website.com|buttons-for-your-website.com|social-buttons.com|free-share-buttons.com|best-seo-offer.com|best-seo-solution.com|buy-cheap-online.info|24x7-allrequestsallowed.com" { url.access-deny =  ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

### Block spam crawlers!
# Empty UA, or -.
#This blocks a lot of spam crawlers and bad robots. 
$HTTP["useragent"] == "" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
$HTTP["useragent"] == "-" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# cURL and Wget/HTTP Libraries commonly used by skiddies (disabled by default).
#$HTTP["useragent"] =~ "curl|libcurl|Wget" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
#$HTTP["useragent"] =~ "libwww-perl|Nmap\ Scripting\ Engine|Microsoft-WebDAV-MiniRedir|Mozilla\ FireFox\ Test" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Deny all URLs to malicious UAs (minimizing access to potentially vulnerable apps).
$HTTP["useragent"] =~ "perl|\{|\}|China.Z|ZmEu|Zollard" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Spammy SEO companies
# Some UAs sourced from http://www.iplists.com/non_engines.txt
# Thunderstone
# NetCraft http://www.netcraft.com/
# MJ12, used by majecticseo.com https://www.majestic12.co.uk/projects/dsearch/mj12bot.php
# 5118.com
# Lightspeed Systems
# IPS-agent by VeriSign http://www.spambotsecurity.com/forum/viewtopic.php?f=7&t=1453
# UA "TurnitinBot/1.4 http://www.turnitin.com/robot/crawlerinfo.html"
# UA "WebZIP/4.0 (http://www.spidersoft.com)"
# Indy Library, a scanner commonly used to collect emails and personal information.
# Ahrefsbot http://ahrefs.com/robot/
$HTTP["useragent"] =~ "thunderstone|PageAnalyzer|Netcraft|5118.com|MJ12bot|dotbot|LSSRocketCrawler|LightspeedSystems|StatsInfo|ips-agent|TurnitinBot|SlySearch|ScoutAbout|RPT-HTTPClient|WebZIP|Webclipping.com|webrank|websquash.com|lwp-trivial|AESOP_com_SpiderMan|Zeus|Cyveillance|Lite\ Bot|flunky|Microsoft\ URL\ Control|NAMEPROTECT|NPBot|aipbot|WebFilter|InfoPath|Indy\ Library|SurveyBot|Morfeus\ Fucking\ Scanner|probethenet.com|digitalmole|AhrefsBot" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Net Scanners, including https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "netscan.lekus.su|pdrlabs.net|massscan|ip138.com" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Some of these IPs are also Baidu, but none of them respect robots.txt and 
# simply change the user-agents of so as to circumvent blocking. Of course, we can fix that :)
# Blocked IPs:
# 123.125.71.x and 220.181.108.x - Baidu Spiders
# 202.46.*.* - Blocks bots @ ptr.cnsat.com.cn 
$HTTP["remoteip"] =~ "123\.125\.71\.*|220\.181\.108\.*|202\.46\.*.*" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Google Crawler Validation (disabled by default, uncomment to enable.)
# Unfortuntely, many spambots impersonate Google's web crawler (Googlebot).
#
# This config attempts to validate requests from Google user agents by comparing
# the ip address of the client to a list of known good IP blocks.
# 
# These IP blocks are retrieved through Google's publicly available lists 
# (https://support.google.com/a/answer/60764?hl=en, http://snurps.blogspot.com/2013/10/how-many-ip-addresses-does-google-have.html)
# with the command dig TXT +short _netblocks{,2,3}.google.com | tr ' ' '\n' | grep '^ip4:'
# Note that the whitelist is purposely as coarse as possible to prevent false negatives. 
#$HTTP["useragent"] =~ "Google|Googlebot" {
#  $HTTP["remoteip"] !~ "64\.*\.*\.*|66\.*\.*\.*|72\.*\.*\.*|74\.*\.*\.*|173\.*\.*\.*|207\.*\.*\.*|209\.*\.*\.*|216\.*\.*\.*" {
#    url.access-deny = ( "" )
#    accesslog.filename = "/var/log/lighttpd/spam.log"
#  }
#}

# Baidu, Sogou, YoDao, and Haosou
$HTTP["useragent"] =~ "Baidu|Baiduspider|Sogou|YoudaoBot|YodaoBot|HaoSouSpider|360Spider" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Always allow access to robots.txt, to everyone.
$HTTP["url"] =~ "^/robots.txt"{ url.access-deny = ("disable") }

##################### END Spam Bot Blocking ##########################
